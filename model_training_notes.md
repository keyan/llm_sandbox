# Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

https://arxiv.org/pdf/2104.04473.pdf
https://github.com/NVIDIA/Megatron-LM

- Seems like the go to source for large model training
- Other labs are not sharing details of training systems so how they achieve scale up on training isn't clear


# llama2

https://arxiv.org/pdf/2307.09288.pdf

- already trained with RLHF against anthropic's hh-rlhf dataset
- What does the actual pytorch model code look like?
    - https://github.com/meta-llama/llama/blob/main/llama/model.py
