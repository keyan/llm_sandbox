{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore reimplementation\n",
    "\n",
    "Code written while following along with: https://www.youtube.com/watch?v=PaCmpygFfXo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting approach, creating a feature by counting occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-cjs9YfOG9Tc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0gnpFzx957qu"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "stoi = {ch: i+1 for i, ch in enumerate(string.ascii_lowercase)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwOfp7JR4s9R",
    "outputId": "69234208-5c5f-49d0-cbcd-6ee56a3123e3"
   },
   "outputs": [],
   "source": [
    "words = open('data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KtsIQvvJ7_TD"
   },
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(stoi[ch1])\n",
    "    ys.append(stoi[ch2])\n",
    "    N[ix1, ix2] += 1\n",
    "\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "xs = torch.Tensor(xs).long()\n",
    "ys = torch.Tensor(ys).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NE3U3BrffPQo"
   },
   "outputs": [],
   "source": [
    "ys = ys.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llCqxz3o8PQ8",
    "outputId": "094a9fbf-4598-48d5-9f2f-67818663b511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = P[ix]\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjIgJO6S9BlI",
    "outputId": "f4bfc0c1-e0b9-4ea3-993a-cf40920deaa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized negative log likelihood: 2.4255\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "n = 0\n",
    "for w in words[:3]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    n += 1\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    prob = P[ix1, ix2]\n",
    "    logprob = torch.log(prob)\n",
    "    loss += logprob\n",
    "\n",
    "print(f'normalized negative log likelihood: {-loss/n:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qdUV5rf86bq"
   },
   "source": [
    "# Neural Net approach w/ manual weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n8Gt2VpsdO8g"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "xs_enc = F.one_hot(xs, 27).float()\n",
    "ys_enc = F.one_hot(ys, 27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uMT7F1-HdoN3"
   },
   "outputs": [],
   "source": [
    "W = torch.rand((27, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qml07jRJdUGm",
    "outputId": "d474e7bc-2f50-4df9-a73a-d6d9f8b3acf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2316, 0.2670, 0.5199,  ..., 0.5978, 0.2065, 0.7383],\n",
      "        [0.2000, 0.4254, 0.0926,  ..., 0.7594, 0.0087, 0.7833],\n",
      "        [0.6711, 0.9686, 0.2646,  ..., 0.3547, 0.3227, 0.0031],\n",
      "        ...,\n",
      "        [0.7150, 0.6141, 0.0686,  ..., 0.6213, 0.4833, 0.5559],\n",
      "        [0.5584, 0.6218, 0.3626,  ..., 0.0862, 0.3065, 0.9438],\n",
      "        [0.4465, 0.8683, 0.1276,  ..., 0.2699, 0.7839, 0.6016]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[1.2606, 1.3060, 1.6819,  ..., 1.8181, 1.2294, 2.0925],\n",
      "        [1.2215, 1.5302, 1.0970,  ..., 2.1369, 1.0087, 2.1887],\n",
      "        [1.9565, 2.6342, 1.3029,  ..., 1.4257, 1.3808, 1.0031],\n",
      "        ...,\n",
      "        [2.0442, 1.8481, 1.0710,  ..., 1.8613, 1.6214, 1.7435],\n",
      "        [1.7479, 1.8624, 1.4371,  ..., 1.0900, 1.3587, 2.5698],\n",
      "        [1.5628, 2.3828, 1.1361,  ..., 1.3098, 2.1901, 1.8251]],\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "num = xs.nelement()\n",
    "for i in range(epochs):\n",
    "  # The unnormalized (before activation fxn) output of a neuron is a \"logit\"\n",
    "  # These are not probabilities and therefore hard to interpret\n",
    "  # 18000, 27 @ 27, 27 -> 18000, 27\n",
    "  logits = xs_enc @ W\n",
    "\n",
    "  # Softmax converts from logits to a probability distribution over a discrete\n",
    "  # variable with n possible outputs.\n",
    "  counts = logits.exp()\n",
    "  probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "  # Negative log likelihood\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(f'Loss: {loss:.4f}')\n",
    "\n",
    "  W.grad = None\n",
    "  loss.backward()\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccaun.\n",
      "lormo.\n",
      "kalllie.\n",
      "fara.\n",
      "rilon.\n",
      "lian.\n",
      "janatayn.\n",
      "chemie.\n",
      "etliz.\n",
      "jolem.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    idx = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([idx]), 27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        char = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[char])\n",
    "        idx = char\n",
    "        if idx == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GesFDC5j80y0"
   },
   "source": [
    "# Neural Net approach w/ Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27]) torch.Size([228146, 27])\n"
     ]
    }
   ],
   "source": [
    "print(xs_enc.shape, ys_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "XYPPtT8oA93W"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class BiGramDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(BiGramDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "all_data = BiGramDataset(xs_enc, ys)\n",
    "train_data, dev_data, test_data = random_split(all_data, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "RL5SgZaeNJvS"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_data,batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "n0t8rMfT52OM"
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(27, 27),\n",
    "        \n",
    "        # A more complex model just makes the predictions/loss worse\n",
    "        #nn.ReLU(),\n",
    "        #nn.Linear(512, 512),\n",
    "        #nn.ReLU(),\n",
    "        #nn.Linear(512, 27),\n",
    "        \n",
    "        # NLLL loss fxn requires the network output to be LogSoftmax\n",
    "        # Can remove this layer if using CrossEntropyLoss \n",
    "        nn.LogSoftmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits\n",
    "\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3w7YSKH3BiCt",
    "outputId": "4fc919a2-9a9c-46c1-93f5-923e5d76d1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "---------------------------------\n",
      "loss: 2.4062838554382324 [0 / 182517]\n",
      "loss: 2.3683180809020996 [20000 / 182517]\n",
      "loss: 2.4546115398406982 [40000 / 182517]\n",
      "loss: 2.419706344604492 [60000 / 182517]\n",
      "loss: 2.495737314224243 [80000 / 182517]\n",
      "loss: 2.451923131942749 [100000 / 182517]\n",
      "loss: 2.4929842948913574 [120000 / 182517]\n",
      "loss: 2.4703428745269775 [140000 / 182517]\n",
      "loss: 2.4828622341156006 [160000 / 182517]\n",
      "loss: 2.4792494773864746 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4775272244992466\n",
      "Epoch: 2\n",
      "---------------------------------\n",
      "loss: 2.406228542327881 [0 / 182517]\n",
      "loss: 2.3683016300201416 [20000 / 182517]\n",
      "loss: 2.4544739723205566 [40000 / 182517]\n",
      "loss: 2.4196372032165527 [60000 / 182517]\n",
      "loss: 2.495696783065796 [80000 / 182517]\n",
      "loss: 2.4518816471099854 [100000 / 182517]\n",
      "loss: 2.4929213523864746 [120000 / 182517]\n",
      "loss: 2.470343828201294 [140000 / 182517]\n",
      "loss: 2.482787609100342 [160000 / 182517]\n",
      "loss: 2.479241132736206 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4774933586949888\n",
      "Epoch: 3\n",
      "---------------------------------\n",
      "loss: 2.406179904937744 [0 / 182517]\n",
      "loss: 2.368283271789551 [20000 / 182517]\n",
      "loss: 2.4543445110321045 [40000 / 182517]\n",
      "loss: 2.419577121734619 [60000 / 182517]\n",
      "loss: 2.495661735534668 [80000 / 182517]\n",
      "loss: 2.451843023300171 [100000 / 182517]\n",
      "loss: 2.4928643703460693 [120000 / 182517]\n",
      "loss: 2.4703450202941895 [140000 / 182517]\n",
      "loss: 2.4827194213867188 [160000 / 182517]\n",
      "loss: 2.479234457015991 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4774635667386264\n",
      "Epoch: 4\n",
      "---------------------------------\n",
      "loss: 2.4061367511749268 [0 / 182517]\n",
      "loss: 2.3682639598846436 [20000 / 182517]\n",
      "loss: 2.4542236328125 [40000 / 182517]\n",
      "loss: 2.4195244312286377 [60000 / 182517]\n",
      "loss: 2.4956297874450684 [80000 / 182517]\n",
      "loss: 2.4518074989318848 [100000 / 182517]\n",
      "loss: 2.492812395095825 [120000 / 182517]\n",
      "loss: 2.470346450805664 [140000 / 182517]\n",
      "loss: 2.4826576709747314 [160000 / 182517]\n",
      "loss: 2.4792282581329346 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4774371541064717\n",
      "Epoch: 5\n",
      "---------------------------------\n",
      "loss: 2.4060981273651123 [0 / 182517]\n",
      "loss: 2.3682432174682617 [20000 / 182517]\n",
      "loss: 2.4541103839874268 [40000 / 182517]\n",
      "loss: 2.419478416442871 [60000 / 182517]\n",
      "loss: 2.495602607727051 [80000 / 182517]\n",
      "loss: 2.4517745971679688 [100000 / 182517]\n",
      "loss: 2.4927656650543213 [120000 / 182517]\n",
      "loss: 2.4703478813171387 [140000 / 182517]\n",
      "loss: 2.4826011657714844 [160000 / 182517]\n",
      "loss: 2.4792237281799316 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4774136854254682\n",
      "Epoch: 6\n",
      "---------------------------------\n",
      "loss: 2.4060633182525635 [0 / 182517]\n",
      "loss: 2.3682217597961426 [20000 / 182517]\n",
      "loss: 2.4540038108825684 [40000 / 182517]\n",
      "loss: 2.4194369316101074 [60000 / 182517]\n",
      "loss: 2.4955780506134033 [80000 / 182517]\n",
      "loss: 2.451744556427002 [100000 / 182517]\n",
      "loss: 2.492722749710083 [120000 / 182517]\n",
      "loss: 2.470349073410034 [140000 / 182517]\n",
      "loss: 2.4825494289398193 [160000 / 182517]\n",
      "loss: 2.479219675064087 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.477392766786658\n",
      "Epoch: 7\n",
      "---------------------------------\n",
      "loss: 2.406031608581543 [0 / 182517]\n",
      "loss: 2.368199110031128 [20000 / 182517]\n",
      "loss: 2.453904390335083 [40000 / 182517]\n",
      "loss: 2.419400453567505 [60000 / 182517]\n",
      "loss: 2.4955568313598633 [80000 / 182517]\n",
      "loss: 2.451716661453247 [100000 / 182517]\n",
      "loss: 2.492683172225952 [120000 / 182517]\n",
      "loss: 2.470350742340088 [140000 / 182517]\n",
      "loss: 2.4825022220611572 [160000 / 182517]\n",
      "loss: 2.4792168140411377 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4773741908695386\n",
      "Epoch: 8\n",
      "---------------------------------\n",
      "loss: 2.4060027599334717 [0 / 182517]\n",
      "loss: 2.368176221847534 [20000 / 182517]\n",
      "loss: 2.453810930252075 [40000 / 182517]\n",
      "loss: 2.4193670749664307 [60000 / 182517]\n",
      "loss: 2.4955382347106934 [80000 / 182517]\n",
      "loss: 2.451690196990967 [100000 / 182517]\n",
      "loss: 2.4926464557647705 [120000 / 182517]\n",
      "loss: 2.4703519344329834 [140000 / 182517]\n",
      "loss: 2.4824585914611816 [160000 / 182517]\n",
      "loss: 2.4792144298553467 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.477357480836951\n",
      "Epoch: 9\n",
      "---------------------------------\n",
      "loss: 2.4059760570526123 [0 / 182517]\n",
      "loss: 2.3681528568267822 [20000 / 182517]\n",
      "loss: 2.453723430633545 [40000 / 182517]\n",
      "loss: 2.4193367958068848 [60000 / 182517]\n",
      "loss: 2.4955217838287354 [80000 / 182517]\n",
      "loss: 2.4516661167144775 [100000 / 182517]\n",
      "loss: 2.492612838745117 [120000 / 182517]\n",
      "loss: 2.470353126525879 [140000 / 182517]\n",
      "loss: 2.4824182987213135 [160000 / 182517]\n",
      "loss: 2.479212999343872 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4773424604664678\n",
      "Epoch: 10\n",
      "---------------------------------\n",
      "loss: 2.405951738357544 [0 / 182517]\n",
      "loss: 2.3681294918060303 [20000 / 182517]\n",
      "loss: 2.453641414642334 [40000 / 182517]\n",
      "loss: 2.419309616088867 [60000 / 182517]\n",
      "loss: 2.49550724029541 [80000 / 182517]\n",
      "loss: 2.451643228530884 [100000 / 182517]\n",
      "loss: 2.492581367492676 [120000 / 182517]\n",
      "loss: 2.4703540802001953 [140000 / 182517]\n",
      "loss: 2.4823808670043945 [160000 / 182517]\n",
      "loss: 2.4792122840881348 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4773289949997612\n",
      "Epoch: 11\n",
      "---------------------------------\n",
      "loss: 2.40592885017395 [0 / 182517]\n",
      "loss: 2.368105411529541 [20000 / 182517]\n",
      "loss: 2.453564405441284 [40000 / 182517]\n",
      "loss: 2.4192845821380615 [60000 / 182517]\n",
      "loss: 2.4954946041107178 [80000 / 182517]\n",
      "loss: 2.451622486114502 [100000 / 182517]\n",
      "loss: 2.4925522804260254 [120000 / 182517]\n",
      "loss: 2.4703545570373535 [140000 / 182517]\n",
      "loss: 2.482346296310425 [160000 / 182517]\n",
      "loss: 2.4792118072509766 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4773169082144033\n",
      "Epoch: 12\n",
      "---------------------------------\n",
      "loss: 2.4059081077575684 [0 / 182517]\n",
      "loss: 2.368081569671631 [20000 / 182517]\n",
      "loss: 2.4534919261932373 [40000 / 182517]\n",
      "loss: 2.4192616939544678 [60000 / 182517]\n",
      "loss: 2.495483636856079 [80000 / 182517]\n",
      "loss: 2.4516024589538574 [100000 / 182517]\n",
      "loss: 2.4925248622894287 [120000 / 182517]\n",
      "loss: 2.4703550338745117 [140000 / 182517]\n",
      "loss: 2.482313871383667 [160000 / 182517]\n",
      "loss: 2.479212760925293 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4773059927898906\n",
      "Epoch: 13\n",
      "---------------------------------\n",
      "loss: 2.405888080596924 [0 / 182517]\n",
      "loss: 2.3680577278137207 [20000 / 182517]\n",
      "loss: 2.4534239768981934 [40000 / 182517]\n",
      "loss: 2.419240951538086 [60000 / 182517]\n",
      "loss: 2.495474100112915 [80000 / 182517]\n",
      "loss: 2.4515841007232666 [100000 / 182517]\n",
      "loss: 2.492499351501465 [120000 / 182517]\n",
      "loss: 2.4703550338745117 [140000 / 182517]\n",
      "loss: 2.4822840690612793 [160000 / 182517]\n",
      "loss: 2.4792134761810303 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4772959895755933\n",
      "Epoch: 14\n",
      "---------------------------------\n",
      "loss: 2.405869722366333 [0 / 182517]\n",
      "loss: 2.3680338859558105 [20000 / 182517]\n",
      "loss: 2.4533603191375732 [40000 / 182517]\n",
      "loss: 2.4192216396331787 [60000 / 182517]\n",
      "loss: 2.4954657554626465 [80000 / 182517]\n",
      "loss: 2.451566696166992 [100000 / 182517]\n",
      "loss: 2.4924750328063965 [120000 / 182517]\n",
      "loss: 2.4703550338745117 [140000 / 182517]\n",
      "loss: 2.4822559356689453 [160000 / 182517]\n",
      "loss: 2.479214906692505 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.477286960767663\n",
      "Epoch: 15\n",
      "---------------------------------\n",
      "loss: 2.4058525562286377 [0 / 182517]\n",
      "loss: 2.3680102825164795 [20000 / 182517]\n",
      "loss: 2.4533002376556396 [40000 / 182517]\n",
      "loss: 2.419203519821167 [60000 / 182517]\n",
      "loss: 2.4954586029052734 [80000 / 182517]\n",
      "loss: 2.451550006866455 [100000 / 182517]\n",
      "loss: 2.492452621459961 [120000 / 182517]\n",
      "loss: 2.4703547954559326 [140000 / 182517]\n",
      "loss: 2.482229471206665 [160000 / 182517]\n",
      "loss: 2.4792163372039795 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4772787197776465\n",
      "Epoch: 16\n",
      "---------------------------------\n",
      "loss: 2.4058358669281006 [0 / 182517]\n",
      "loss: 2.3679869174957275 [20000 / 182517]\n",
      "loss: 2.45324444770813 [40000 / 182517]\n",
      "loss: 2.419187307357788 [60000 / 182517]\n",
      "loss: 2.4954521656036377 [80000 / 182517]\n",
      "loss: 2.4515347480773926 [100000 / 182517]\n",
      "loss: 2.492431163787842 [120000 / 182517]\n",
      "loss: 2.4703540802001953 [140000 / 182517]\n",
      "loss: 2.4822049140930176 [160000 / 182517]\n",
      "loss: 2.4792182445526123 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.477271256239518\n",
      "Epoch: 17\n",
      "---------------------------------\n",
      "loss: 2.405820846557617 [0 / 182517]\n",
      "loss: 2.3679633140563965 [20000 / 182517]\n",
      "loss: 2.4531915187835693 [40000 / 182517]\n",
      "loss: 2.4191718101501465 [60000 / 182517]\n",
      "loss: 2.4954466819763184 [80000 / 182517]\n",
      "loss: 2.4515199661254883 [100000 / 182517]\n",
      "loss: 2.492410898208618 [120000 / 182517]\n",
      "loss: 2.470353364944458 [140000 / 182517]\n",
      "loss: 2.4821815490722656 [160000 / 182517]\n",
      "loss: 2.4792206287384033 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4772644250289253\n",
      "Epoch: 18\n",
      "---------------------------------\n",
      "loss: 2.405806064605713 [0 / 182517]\n",
      "loss: 2.3679404258728027 [20000 / 182517]\n",
      "loss: 2.453141689300537 [40000 / 182517]\n",
      "loss: 2.4191577434539795 [60000 / 182517]\n",
      "loss: 2.4954421520233154 [80000 / 182517]\n",
      "loss: 2.4515061378479004 [100000 / 182517]\n",
      "loss: 2.492391586303711 [120000 / 182517]\n",
      "loss: 2.4703519344329834 [140000 / 182517]\n",
      "loss: 2.4821596145629883 [160000 / 182517]\n",
      "loss: 2.4792237281799316 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4772582261458687\n",
      "Epoch: 19\n",
      "---------------------------------\n",
      "loss: 2.405792474746704 [0 / 182517]\n",
      "loss: 2.367917537689209 [20000 / 182517]\n",
      "loss: 2.4530951976776123 [40000 / 182517]\n",
      "loss: 2.419144630432129 [60000 / 182517]\n",
      "loss: 2.4954380989074707 [80000 / 182517]\n",
      "loss: 2.451493263244629 [100000 / 182517]\n",
      "loss: 2.49237322807312 [120000 / 182517]\n",
      "loss: 2.470350742340088 [140000 / 182517]\n",
      "loss: 2.4821391105651855 [160000 / 182517]\n",
      "loss: 2.4792261123657227 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.4772524108057437\n",
      "Epoch: 20\n",
      "---------------------------------\n",
      "loss: 2.4057793617248535 [0 / 182517]\n",
      "loss: 2.3678951263427734 [20000 / 182517]\n",
      "loss: 2.4530513286590576 [40000 / 182517]\n",
      "loss: 2.4191324710845947 [60000 / 182517]\n",
      "loss: 2.4954347610473633 [80000 / 182517]\n",
      "loss: 2.4514803886413574 [100000 / 182517]\n",
      "loss: 2.492356061935425 [120000 / 182517]\n",
      "loss: 2.4703493118286133 [140000 / 182517]\n",
      "loss: 2.48211932182312 [160000 / 182517]\n",
      "loss: 2.479229211807251 [180000 / 182517]\n",
      "test erorr -- acc: 22.388779311856236, avg loss: 2.477247175963029\n",
      "test erorr -- acc: 22.196896642412554, avg loss: 2.4661297072534976\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    pred = model(X.to(device))\n",
    "    loss = loss_fn(pred, y.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 20 == 0:\n",
    "      loss, current = loss.item(), batch * batch_size\n",
    "      print(f'loss: {loss} [{current} / {size}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      X,y = X.to(device), y.to(device)\n",
    "      pred = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f'test erorr -- acc: {100*correct}, avg loss: {test_loss}')\n",
    "\n",
    "\n",
    "learning_rate = 10\n",
    "epochs = 20\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "  print(f'Epoch: {t+1}\\n---------------------------------')\n",
    "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "  # Validation test\n",
    "  test_loop(dev_dataloader, model, loss_fn)\n",
    "\n",
    "# Final test\n",
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khonn.\n",
      "tyar.\n",
      "thofrency.\n",
      "lanahazan.\n",
      "kyon.\n",
      "juri.\n",
      "ama.\n",
      "aeiri.\n",
      "rissaldamahrahleh.\n",
      "jeshillil.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(15):\n",
    "    idx = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([idx]), 27).float()\n",
    "        probs = model(xenc.to(device)).exp()\n",
    "        \n",
    "        char = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[char])\n",
    "        idx = char\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    if len(out) > 3:\n",
    "        print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
